{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyMT3wv0JhDfJy8gtWXiKZti"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"5BSp84nHA0MB","colab_type":"code","outputId":"e8daf98d-87a9-4d12-e5ef-47e33dec8d30","executionInfo":{"status":"error","timestamp":1588340051899,"user_tz":-330,"elapsed":6669,"user":{"displayName":"Rising_Star","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9SU6T-ZqIFXctaT-q-oTSZfcPZnIGe_aH9uaoUw=s64","userId":"06547433164503958576"}},"colab":{"base_uri":"https://localhost:8080/","height":362}},"source":["import argparse\n","from tqdm import tqdm\n","from sklearn.metrics import confusion_matrix, f1_score\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","from torchvision.utils import make_grid\n","from tensorboardX import SummaryWriter\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","#import models\n","from utils.occlusion import occlusion\n","\n","model_names = sorted(name for name in models.__dict__ if not name.startswith(\"__\") and callable(models.__dict__[name]))\n","\n","parser = argparse.ArgumentParser(description='CNN')\n","parser.add_argument('--device', type=str, default='0', help='GPU device (default: 0)')\n","parser.add_argument('--dataset', default='CHASE', choices=['DRIVE', 'CHASE'])\n","parser.add_argument('--data_path', type=str, default='datasets/', help='data path')\n","parser.add_argument('--model', type=str, default='FCN', choices=model_names,\n","                    help='model architecture: ' + ' | '.join(model_names) + ' (default: FCN)')\n","parser.add_argument('--batch_size', type=int, default=1024, help='input batch size for training (default: 128)')\n","parser.add_argument('--epochs', type=int, default=200, help='number of epochs to train (default: 300)')\n","parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n","parser.add_argument('--lr_decay', action='store_true', default=False, help='learning rate decay')\n","parser.add_argument('--threshold_confusion', default=0.5, type=float, help='threshold_confusion')\n","parser.add_argument('--seed', type=int, default=1234, help='random seed (default: 1234)')\n","\n","parser.add_argument('--patch_num', type=int, default=204800, help='patchs number (default: 800000)')\n","parser.add_argument('--patch_size', type=int, default=48, help='patch size (default: 48)')\n","\n","parser.add_argument('--data_augmentation', action='store_true', default=False, help='data augmentation')\n","parser.add_argument('--occlusion', action='store_true', default=False, help='is add occlusion?')\n","parser.add_argument('--occ_p', default=0.5, type=float, help='occlusion prob')\n","parser.add_argument('--occ_length', type=int, default=24, help='length of the occlusion')\n","parser.add_argument('--occ_func', default='fill_next_tar', choices=['fill_0', 'fill_R', 'fill_next'], help='occ_func')\n","\n","# use last save model\n","parser.add_argument('--load_last', action='store_true', default=False, help='load last model')\n","parser.add_argument('--load_path', type=str, default='logs/', help='load model path')\n","parser.add_argument('--logs_path', type=str, default='logs/', help='load model path')\n","\n","args = vars(parser.parse_args())\n","os.environ['CUDA_VISIBLE_DEVICES'] = args['device']\n","cudnn.benchmark = True\n","torch.cuda.is_available()\n","torch.manual_seed(args['seed'])\n","torch.cuda.manual_seed(args['seed'])\n","\n","threshold_confusion = args['threshold_confusion']\n","\n","if str(args['logs_path']).endswith('/') is False:\n","    args['logs_path'] += '/'\n","\n","if args['load_path'] is not None and str(args['load_path']).endswith('/') is False:\n","    args['load_path'] += '/'\n","\n","if args['load_last'] is False:\n","    mkdir_p(args['logs_path'] + args['dataset'] + '/' + args['model'] + '/')\n","    index = np.sort(np.array(os.listdir(args['logs_path'] + args['dataset'] + '/' + args['model'] + '/'), dtype=int))\n","    index = index.max() + 1 if len(index) > 0 else 1\n","    basic_path = args['logs_path'] + args['dataset'] + '/' + args['model'] + '/' + str(index) + '/'\n","    mkdir_p(basic_path)\n","    args['load_path'] = basic_path\n","    max_acc, max_F1_score, max_sensitivity = 0., 0., 0.\n","    cur_epoch = 0\n","    logs = []\n","    logs.append(['epoch', 'test_acc', 'max_acc', 'specificity', 'sensitivity', 'max_sensitivity', 'F1_score', 'max_F1_score'])\n","else:\n","    basic_path = args['load_path']\n","    assert os.path.exists(basic_path), '目录不存在'\n","    assert os.path.isfile(basic_path + 'checkpoints/last.pt'), 'Error: no checkpoint file found!'\n","    checkpoint = torch.load(basic_path + 'checkpoints/last.pt')\n","    checkpoint['args']['load_last'] = args['load_last']\n","    checkpoint['args']['load_path'] = args['load_path']\n","    args = checkpoint['args']\n","    max_acc = checkpoint['max_acc']\n","    max_sensitivity = checkpoint['max_sensitivity']\n","    max_F1_score = checkpoint['max_F1_score']\n","    cur_epoch = checkpoint['epoch'] + 1\n","    logs = checkpoint['logs']\n","    print('保存模型的最后一次训练结果： %s, 当前训练周期: %4d, ' % (str(logs[-1]), cur_epoch))\n","    assert cur_epoch < args['epochs'], '已经跑完了，cur_epoch: {}，epochs: {}'.format(cur_epoch, args['epochs'])\n","\n","print('当前日志目录： ' + basic_path)\n","mkdir_p(basic_path + 'checkpoints/periods/')\n","mkdir_p(basic_path + 'tensorboard/')\n","print(args)\n","with open(basic_path + 'args.txt', 'w+') as f:\n","    for arg in args:\n","        f.write(str(arg) + ': ' + str(args[arg]) + '\\n')\n","\n","vis = get_visdom()\n","if vis is not None:\n","    import time\n","\n","    vis.env = args['dataset'] + '_' + args['model'] + '_' + time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime(time.time()))\n","\n","\n","net = models.__dict__[args['model']]().cuda()\n","criterion = nn.CrossEntropyLoss().cuda()\n","optimizer = optim.Adam(net.parameters(), lr=args['lr'])\n","\n","if args['load_last'] is True and cur_epoch > 0:\n","    net.load_state_dict(checkpoint['net'], strict=False)\n","    print('load path: ' + basic_path + 'checkpoints/last.pt')\n","\n","# 加载数据集\n","train_orig_imgs, train_orig_gts, train_orig_masks, test_orig_imgs, test_orig_gts, test_orig_masks = get_orig_datasets(args)\n","test_imgs, test_gts, test_imgs_patches, test_masks_patches = get_testing_patchs(\n","    test_imgs=test_orig_imgs,\n","    test_gts=test_orig_gts,\n","    patch_size=args['patch_size'],\n",")\n","test_set = TestDataset(test_imgs_patches)\n","test_loader = DataLoader(test_set, batch_size=args['batch_size'], shuffle=False, num_workers=0)\n","\n","ts_writer = SummaryWriter(log_dir=basic_path + 'tensorboard/', comment=args['model'])\n","args_str = ''\n","for arg in args:\n","    args_str += str(arg) + ': ' + str(args[arg]) + '<br />'\n","ts_writer.add_text('args', args_str, cur_epoch)\n","\n","if args['lr_decay'] is True:\n","    scheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=10, verbose=True)\n","\n","org_images, org_targets, occ_images, occ_targets, vis_mask, vis_outputs = None, None, None, None, None, None\n","\n","\n","def train():\n","    global org_images, org_targets, occ_images, occ_targets, vis_mask, vis_outputs, max_acc, max_F1_score, max_sensitivity\n","    for epoch in range(cur_epoch, args['epochs']):\n","        if args['lr_decay'] is True:\n","            scheduler.step(max_F1_score)\n","        # train network\n","        train_loss = 0\n","        train_imgs_patches, train_masks_patches = get_training_patchs(\n","            train_imgs=train_orig_imgs,\n","            train_gts=train_orig_gts,\n","            patch_size=args['patch_size'],\n","            patch_num=args['patch_num']\n","        )\n","        train_set = TrainDataset(train_imgs_patches, train_masks_patches, data_augmentation=args['data_augmentation'])\n","        train_loader = DataLoader(train_set, batch_size=args['batch_size'], shuffle=True, num_workers=0)\n","        progress_bar = tqdm(train_loader)\n","        net.train()\n","        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n","            progress_bar.set_description('Epoch {}-{}'.format(epoch + 1, args['epochs']))\n","\n","            if vis is not None and batch_idx % 5 == 0:\n","                org_images = vis.image(make_grid(inputs.data[0:64], nrow=32, normalize=True, scale_each=True, padding=4, pad_value=1),\n","                                       opts=dict(title='Original Images'),\n","                                       win=org_images)\n","                org_targets = vis.image(\n","                    make_grid(targets[0:64].type_as(torch.FloatTensor()).view(64, 1, args['patch_size'], args['patch_size']), nrow=32,\n","                              normalize=True,\n","                              scale_each=True, padding=4, pad_value=1),\n","                    opts=dict(title='Original Targets'),\n","                    win=org_targets)\n","\n","            if args['occlusion'] is True:\n","                inputs, targets = occlusion(inputs, targets, args['occ_length'], args['occ_func'], args['occ_p'])\n","                if vis is not None and batch_idx % 5 == 0:\n","                    occ_images = vis.image(make_grid(inputs.data[0:64], nrow=32, normalize=True, scale_each=True, padding=4, pad_value=1),\n","                                           opts=dict(title='Occlusion Images'),\n","                                           win=occ_images)\n","                    occ_targets = vis.image(\n","                        make_grid(targets[0:64].type_as(torch.FloatTensor()).view(64, 1, args['patch_size'], args['patch_size']), nrow=32,\n","                                  padding=4,\n","                                  pad_value=1),\n","                        opts=dict(title='Occlusion Targets'),\n","                        win=occ_targets)\n","\n","            inputs = Variable(inputs.cuda().detach())\n","            targets = Variable(targets.cuda().detach())\n","\n","            optimizer.zero_grad()\n","            \n","            output = net(inputs)\n","\n","            loss = criterion(output, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            progress_bar.set_postfix(loss='%.3f' % (train_loss / (batch_idx + 1)))\n","\n","\n","if __name__ == '__main__':\n","    train()\n","    ts_writer.close()"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-906edeae2153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboardX'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"D3oIUqN9A4e-","colab_type":"code","outputId":"416e57f4-1102-40d3-80a8-6970685f136c","executionInfo":{"status":"ok","timestamp":1588181923459,"user_tz":-330,"elapsed":8152,"user":{"displayName":"Rising_Star","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9SU6T-ZqIFXctaT-q-oTSZfcPZnIGe_aH9uaoUw=s64","userId":"06547433164503958576"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["!pip install tensorboardX"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n","\u001b[K     |████████████████████████████████| 204kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.1.3)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6Tm8NnkMBNYr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}