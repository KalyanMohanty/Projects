{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"numpy.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNoAmP4mK++xu6h+zqsFV/7"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AOW0ekGBn2kG","colab_type":"text"},"source":["# Sigmoid Function"]},{"cell_type":"code","metadata":{"id":"0um1M8tohwlp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"71d4bbeb-314a-45c5-b69e-d801391a428b","executionInfo":{"status":"ok","timestamp":1592218809351,"user_tz":-330,"elapsed":800,"user":{"displayName":"Rising_Star","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9SU6T-ZqIFXctaT-q-oTSZfcPZnIGe_aH9uaoUw=s64","userId":"06547433164503958576"}}},"source":["import math\n","import numpy as np\n","\n","def sigmoid(x):\n","\n","  s = 1/(1+np.exp(-x))\n","  return s\n","sigmoid(5)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9933071490757153"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"LBO9x5q6iEYx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"f0958564-acfb-47cb-f054-10c21f28d483","executionInfo":{"status":"ok","timestamp":1592218867903,"user_tz":-330,"elapsed":1751,"user":{"displayName":"Rising_Star","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9SU6T-ZqIFXctaT-q-oTSZfcPZnIGe_aH9uaoUw=s64","userId":"06547433164503958576"}}},"source":["x = np.array([1,2,3])\n","sigmoid(x)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.73105858, 0.88079708, 0.95257413])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"vNhTd-09pjE-","colab_type":"text"},"source":["# Sigmoid gradient\n","- sigmoid_derivative(x)=σ′(x)=σ(x)(1−σ(x))\n","-   σ′(x)=s(1−s)σ′(x)=s(1−s)\n"]},{"cell_type":"code","metadata":{"id":"3kosFBWwkpb_","colab_type":"code","outputId":"ead1eac0-b94f-46e9-d45c-4ecdd059c9a1","executionInfo":{"status":"ok","timestamp":1592219396192,"user_tz":-330,"elapsed":1318,"user":{"displayName":"Rising_Star","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9SU6T-ZqIFXctaT-q-oTSZfcPZnIGe_aH9uaoUw=s64","userId":"06547433164503958576"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["def sigmoid_derivative(x):\n","\n","  s = 1/(1+np.exp(-x))\n","  ds = s * (s-1)\n","  return ds\n","print('sigmoid derivative(x) = '+ str(sigmoid_derivative(x)))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["sigmoid derivative(x) = [-0.19661193 -0.10499359 -0.04517666]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"enOJ04nJrmOC","colab_type":"text"},"source":["#  Reshaping arrays"]},{"cell_type":"code","metadata":{"id":"5KYm9z_5kz_0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":468},"outputId":"77085071-0fe2-486f-968d-254b764cc652","executionInfo":{"status":"ok","timestamp":1592219825398,"user_tz":-330,"elapsed":803,"user":{"displayName":"Rising_Star","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9SU6T-ZqIFXctaT-q-oTSZfcPZnIGe_aH9uaoUw=s64","userId":"06547433164503958576"}}},"source":["def image2vector(image):\n","  v = image.reshape(image.shape[0] * image.shape[1], image.shape[2], 1)\n","  return v\n","image = np.array([[[ 0.67826139,  0.29380381],\n","        [ 0.90714982,  0.52835647],\n","        [ 0.4215251 ,  0.45017551]],\n","\n","       [[ 0.92814219,  0.96677647],\n","        [ 0.85304703,  0.52351845],\n","        [ 0.19981397,  0.27417313]],\n","\n","       [[ 0.60659855,  0.00533165],\n","        [ 0.10820313,  0.49978937],\n","        [ 0.34144279,  0.94630077]]])\n","print('Image to vector :' + str(image2vector(image)))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Image to vector :[[[0.67826139]\n","  [0.29380381]]\n","\n"," [[0.90714982]\n","  [0.52835647]]\n","\n"," [[0.4215251 ]\n","  [0.45017551]]\n","\n"," [[0.92814219]\n","  [0.96677647]]\n","\n"," [[0.85304703]\n","  [0.52351845]]\n","\n"," [[0.19981397]\n","  [0.27417313]]\n","\n"," [[0.60659855]\n","  [0.00533165]]\n","\n"," [[0.10820313]\n","  [0.49978937]]\n","\n"," [[0.34144279]\n","  [0.94630077]]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZXHxl4IVtWUm","colab_type":"text"},"source":["# Normalizing rows\n","### 1.4 - Normalizing rows\n","\n","Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n","\n","For example, if $$x = \n","\\begin{bmatrix}\n","    0 & 3 & 4 \\\\\n","    2 & 6 & 4 \\\\\n","\\end{bmatrix}\\tag{3}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n","    5 \\\\\n","    \\sqrt{56} \\\\\n","\\end{bmatrix}\\tag{4} $$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n","    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n","    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n","\\end{bmatrix}\\tag{5}$$ Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you're going to learn about it in part 5.\n"]},{"cell_type":"code","metadata":{"id":"iMZaJIx_shbr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"45d29f45-8888-4fab-9fc3-42a95f0fa454","executionInfo":{"status":"ok","timestamp":1592220347934,"user_tz":-330,"elapsed":1388,"user":{"displayName":"Rising_Star","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9SU6T-ZqIFXctaT-q-oTSZfcPZnIGe_aH9uaoUw=s64","userId":"06547433164503958576"}}},"source":["def normalizeRows(x):\n","  x_norm = np.linalg.norm(x, axis =1, keepdims=True)\n","  x = x/x_norm\n","  return x\n","x = np.array([\n","    [0, 3, 4],\n","    [1, 6, 4]])\n","print('Normalized rows:' +str(normalizeRows(x)))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Normalized rows:[[0.         0.6        0.8       ]\n"," [0.13736056 0.82416338 0.54944226]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EH7UxZ1uvnys","colab_type":"text"},"source":["# Broadcasting and the softmax function"]},{"cell_type":"code","metadata":{"id":"w-GqZJT7s1Os","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"cc6a18ae-58e0-4c0a-9c71-1208fcf411a0","executionInfo":{"status":"ok","timestamp":1592220750853,"user_tz":-330,"elapsed":932,"user":{"displayName":"Rising_Star","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9SU6T-ZqIFXctaT-q-oTSZfcPZnIGe_aH9uaoUw=s64","userId":"06547433164503958576"}}},"source":["def softmax(x):\n","  x_exp = np.exp(x)\n","  x_sum = np.sum(x_exp, axis =1 , keepdims=True)\n","  s = x_exp/x_sum\n","  return s\n","x = np.array([\n","    [9, 2, 5, 0, 0],\n","    [7, 5, 0, 0 ,0]])\n","print(\"softmax(x) = \" + str(softmax(x)))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n","  1.21052389e-04]\n"," [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n","  8.01252314e-04]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OWpYn7GExIEH","colab_type":"text"},"source":["# Vectorization"]},{"cell_type":"code","metadata":{"id":"AfhUYAx3wI-k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":399},"outputId":"b4f57d57-209b-4cad-eb62-4725ed2e9226","executionInfo":{"status":"ok","timestamp":1592221078321,"user_tz":-330,"elapsed":1534,"user":{"displayName":"Rising_Star","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9SU6T-ZqIFXctaT-q-oTSZfcPZnIGe_aH9uaoUw=s64","userId":"06547433164503958576"}}},"source":["import time\n","\n","x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n","x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n","\n","### VECTORIZED DOT PRODUCT OF VECTORS ###\n","tic = time.process_time()\n","dot = np.dot(x1,x2)\n","toc = time.process_time()\n","print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### VECTORIZED OUTER PRODUCT ###\n","tic = time.process_time()\n","outer = np.outer(x1,x2)\n","toc = time.process_time()\n","print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n","tic = time.process_time()\n","mul = np.multiply(x1,x2)\n","toc = time.process_time()\n","print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### VECTORIZED GENERAL DOT PRODUCT ###\n","tic = time.process_time()\n","W = np.random.rand(3,len(x1))\n","dot = np.dot(W,x1)\n","toc = time.process_time()\n","print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"],"execution_count":26,"outputs":[{"output_type":"stream","text":["dot = 278\n"," ----- Computation time = 0.07194900000051518ms\n","outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n"," [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n"," [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n"," [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n"," [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n"," [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"," ----- Computation time = 0.1326930000002946ms\n","elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n"," ----- Computation time = 0.052112000000548164ms\n","gdot = [19.52226367 28.30205875 27.70016121]\n"," ----- Computation time = 0.879732000000466ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zUSpczE5xfIu","colab_type":"text"},"source":["# Loss function L1 and L2\n","\n","- The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n","- L1 loss is defined as:\n","$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"]},{"cell_type":"code","metadata":{"id":"BUN5fSlcxO0J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"81219bc3-6b75-4df4-a5d8-067234c3fc7d","executionInfo":{"status":"ok","timestamp":1592221229283,"user_tz":-330,"elapsed":1527,"user":{"displayName":"Rising_Star","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9SU6T-ZqIFXctaT-q-oTSZfcPZnIGe_aH9uaoUw=s64","userId":"06547433164503958576"}}},"source":["def L1(yhat, y):\n","  loss = np.sum(np.abs(yhat-y))\n","  return loss\n","yhat = np.array([.9, 0.2, 0.1, .4, .9])\n","y = np.array([1, 0, 0, 1, 1])\n","print(\"L1 = \" + str(L1(yhat,y)))"],"execution_count":29,"outputs":[{"output_type":"stream","text":["L1 = 1.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AiuWq9rvyDFO","colab_type":"text"},"source":["- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"]},{"cell_type":"code","metadata":{"id":"GiLBzYUqx5IE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"834706d3-136e-4bc1-d791-a1e612b62403","executionInfo":{"status":"ok","timestamp":1592221348061,"user_tz":-330,"elapsed":1322,"user":{"displayName":"Rising_Star","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9SU6T-ZqIFXctaT-q-oTSZfcPZnIGe_aH9uaoUw=s64","userId":"06547433164503958576"}}},"source":["def L2(yhat, y):\n","  loss = np.sum(np.power((yhat-y),2))\n","  return loss\n","yhat = np.array([.9, 0.2, 0.1, .4, .9])\n","y = np.array([1, 0, 0, 1, 1])\n","print(\"L2 = \" + str(L2(yhat,y)))\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["L2 = 0.43\n"],"name":"stdout"}]}]}